{
	// Place your snippets for python here. Each snippet is defined under a snippet name and has a prefix, body and 
	// description. The prefix is what is used to trigger the snippet and the body will be expanded and inserted. Possible variables are:
	// $1, $2 for tab stops, $0 for the final cursor position, and ${1:label}, ${2:another} for placeholders. Placeholders with the 
	// same ids are connected.
	// Example:
	// "Print to console": {
	// 	"prefix": "log",
	// 	"body": [
	// 		"console.log('$1');",
	// 		"$2"
	// 	],
	// 	"description": "Log output to console"
	// }
	"main": {
		"prefix": "main",
		"body": [
			"if __name__ == '__main__':",
			"    ${1:pass}"
		],
		"description": "main"
	},
	"import numpy as np": {
		"prefix": "import numpy as np",
		"body": [
			"import numpy as np",
		],
		"description": "import numpy as np"
	},
	"with open(filepath, 'rb') as f": {
		"prefix": "with open(filepath, 'rb') as f",
		"body": [
			"with open(filepath, 'rb') as f",
			"    f",
		],
		"description": "with open(filepath, 'rb') as f"
	},
	"datetime.now().strftime('%Y%m%d_%H%M%S')": {
		"prefix": "datetime.now().strftime('%Y%m%d_%H%M%S')",
		"body": [
			"datetime.now().strftime('%Y%m%d_%H%M%S')",
		],
		"description": "datetime.now().strftime('%Y%m%d_%H%M%S')"
    },
	"from glob import glob": {
		"prefix": "from glob import glob",
		"body": [
			"from glob import glob",
		],
		"description": "from glob import glob"
	},
	"glob_dir": {
		"prefix": "sorted glob(dir_name+os.sep+'*')",
		"body": [
			"sorted(glob(dir_name+os.sep+'*'))",
		],
		"description": "glob_dir"
	},
	"glob recursive": {
		"prefix": "sorted glob(dir_name+os.sep+'*/*', recursive=True)",
		"body": [
			"sorted(glob(dir_name+os.sep+'*/*', recursive=True))",
		],
		"description": "glob recursive"
	},
	"sorted glob_dir": {
		"prefix": "sorted(glob(dir_name+os.sep+'*'))",
		"body": [
			"sorted(glob(dir_name+os.sep+'*'))",
		],
		"description": "sorted glob_dir"
	},
	"lgb": {
        "prefix": [
            "lgb",
            "import lightgbm as lgb"
        ],
        "body": "import lightgbm as lgb",
        "description": "Import LightGBM"
    },
    "np": {
        "prefix": [
            "np",
            "import numpy as np"
        ],
        "body": "import numpy as np",
        "description": "Import Numpy"
    },
    "pd": {
        "prefix": [
            "pd",
            "import pandas as pd"
        ],
        "body": "import pandas as pd",
        "description": "Import Pandas"
    },
    "plt": {
        "prefix": [
            "plt",
            "import matplotlib.pyplot as plt",
            "from matplotlib import ..."
        ],
        "body": "from matplotlib import pyplot as plt",
        "description": "Import Matplotlib"
    },
    "sns": {
        "prefix": [
            "sns",
            "import seaborn as sns"
        ],
        "body": "import seaborn as sns",
        "description": "Import seaborn"
    },
    "joblib.dump": {
        "prefix": [
            "joblib.dump",
            "from joblib import dump"
        ],
        "body": "from joblib import dump",
        "description": "Import `dump` in Joblib"
    },
    "joblib.load": {
        "prefix": [
            "joblib.load",
            "from joblib import load"
        ],
        "body": "from joblib import load",
        "description": "Import `load` in Joblib"
    },
    "sklearn.compose.make_column_transformer": {
        "prefix": [
            "sklearn.compose.make_column_transformer",
            "from sklearn.compose import ..."
        ],
        "body": "from sklearn.compose import make_column_transformer",
        "description": "Import `make_column_transformer` in scikit-learn"
    },
    "sklearn.datasets.load_*": {
        "prefix": [
            "sklearn.datasets.load_*",
            "from sklearn.datasets import ..."
        ],
        "body": "from sklearn.datasets import ${1:load_iris}",
        "description": "Import a function that loads a dataset"
    },
    "sklearn.pipeline.make_pipeline": {
        "prefix": [
            "sklearn.pipeline.make_pipeline",
            "from sklearn.pipeline import ..."
        ],
        "body": "from sklearn.pipeline import make_pipeline",
        "description": "Import `make_pipeline` in scikit-learn"
    },
    "logger = ...": {
        "prefix": "logger = ...",
        "body": "logger = logging.getLogger(${1:__name__})",
        "description": "Get a logger"
    },
    "dtrain = ...": {
        "prefix": "dtrain = ...",
        "body": "dtrain = lgb.Dataset(${1:X}, label=${2:y})",
        "description": "Create a LightGBM dataset instance"
    },
    "booster = ...": {
        "prefix": "booster = ...",
        "body": [
            "booster = lgb.train(",
            "\t${1:params},",
            "\t${2:dtrain},",
            "\t${3:# **kwargs}",
            ")"
        ],
        "description": "Train a LightGBM booster"
    },
    "ax = ...": {
        "prefix": "ax = ...",
        "body": [
            "ax = lgb.plot_importance(",
            "\t${1:booster},",
            "\t${2:# **kwargs}",
            ")"
        ],
        "description": "Plot feature importances"
    },
    "f, ax = ...": {
        "prefix": "f, ax = ...",
        "body": "f, ax = plt.subplots(figsize=${1:(8, 6)})",
        "description": "Create a figure and a set of subplots"
    },
    "df = ...": {
        "prefix": "df = ...",
        "body": [
            "df = pd.read_csv(",
            "\t${1:filepath_or_buffer},",
            "\t${2:# **kwargs}",
            ")"
        ],
        "description": "Read a csv file into a Pandas dataFrame"
    },
    "description = ...": {
        "prefix": "description = ...",
        "body": "description = ${1:df}.describe(include=${2:\"all\"})",
        "description": "Create a Pandas dataframe description"
    },
    "with pd.option_context(...": {
        "prefix": "with pd.option_context(...",
        "body": [
            "with.pd.option_context(",
            "\t\"display.max_rows\",",
            "\t${1:None},",
            "\t\"display.max_columns\",",
            "\t${2:None},",
            "):",
            "\tdisplay(${3:pass})"
        ],
        "description": "Set temporarily Pandas options"
    },
    "X, y = ...": {
        "prefix": "X, y = ...",
        "body": "X, y = ${1:load_iris}(return_X_y=True)",
        "description": "Load and return the dataset"
    },
    "from sklearn.model_selection import train_test_split": {
        "prefix": "from sklearn.model_selection import train_test_split",
        "body": "from sklearn.model_selection import train_test_split",
        "description": "from sklearn.model_selection import train_test_split"
    },
    "X_train, X_test, ...": {
        "prefix": "X_train, X_test, ...",
        "body": [
            "X_train, X_test, y_train, y_test = train_test_split(",
            "\tX,",
            "\ty,",
            "\trandom_state=${1:0},",
            "\tshuffle=${2:True},",
            ")"
        ],
        "description": "Split arrays into train and test subsets"
    },
    "estimator = BaseEstimator(...": {
        "prefix": "estimator = BaseEstimator(...",
        "body": [
            "estimator = ${1:BaseEstimator}(",
            "\t${2:# **params}",
            ")"
        ],
        "description": "Create an scikit-learn estimator instance"
    },
    "estimator = make_pipeline(...": {
        "prefix": "estimator = make_pipeline(...",
        "body": [
            "estimator = make_pipeline(",
            "\t${1:estimator},",
            "\t${2:# *steps}",
            ")"
        ],
        "description": "Create a scikit-learn pipeline instance"
    },
    "estimator = make_column_transformer(...": {
        "prefix": "estimator = make_column_transformer(...",
        "body": [
            "estimator = make_column_transformer(",
            "\t(${1:estimator}, ${2:columns}),",
            "\t${3:# *transformers}",
            ")"
        ],
        "description": "Create a scikit-learn column transformer instance"
    },
    "estimator.fit(...": {
        "prefix": "estimator.fit(...",
        "body": [
            "${1:estimator}.fit(",
            "\t${2:X},",
            "\ty=${3:y},",
            "\t${4:# **fit_params}",
            ")"
        ],
        "description": "Fit the estimator according to the given training data"
    },
    "dump(...": {
        "prefix": "dump(...",
        "body": "dump(${1:estimator}, ${2:filename}, compress=${3:0})",
        "description": "Save the estimator"
    },
    "estimator = load(...": {
        "prefix": "estimator = load(...",
        "body": "estimator = load(${1:filename})",
        "description": "Load the estimator"
    },
    "y_pred = ...": {
        "prefix": "y_pred = ...",
        "body": "y_pred = ${1:estimator}.predict(${2:X})",
        "description": "Predict using the fitted model"
    },
    "X = ...": {
        "prefix": "X = ...",
        "body": "X = ${1:estimator}.transform(${2:X})",
        "description": "Transform the data"
	},
	"MyClass": {
		"prefix": "class",
		"body": [
            "class MyClass:",
			"    def __init__(self):",
			"        self.name = ''",
		],
		"description": "class"
	},
	"argparse": {
		"prefix": "argparse",
		"body": [
            "parser = argparse.ArgumentParser(description='このプログラムの説明', formatter_class=argparse.ArgumentDefaultsHelpFormatter, fromfile_prefix_chars='@')",
			"parser.add_argument('--filename', default=\"\", help='ファイル名.必ず指定する引数にする場合--filename→filenameとする.')",
            "parser.add_argument('--input_dir', default=\".\", help='入力ディレクトリ.必ず指定する引数にする場合--input_dir→input_dirとする.')",
			"parser.add_argument('--seed', type=int,default=4321, help='random seed')",
			"parser.add_argument('-a', '--arg4', type=float)",
			"parser.add_argument('--arg_if_set_true', action='store_true', help='セットすると変数にTrueがセットされる')",
			"parser.add_argument('--arg_if_set_false', action='store_false', help='セットすると変数にFalseがセットされる')",
			"parser.add_argument('-n', '--narg', nargs='*', default=[])",
			"args = parser.parse_args()",
			"pprint(args.__dict__)",
		],
		"description": "argparse"
	},
	"OrderedDict": {
		"prefix": "from collections import OrderedDict",
		"body": [
			"from collections import OrderedDict",
		],
		"description": "OrderedDict"
    },
	"tqdm": {
		"prefix": "tqdm",
		"body": [
			"for index, _ in tqdm(_s):",
            "    print(_)",
		],
		"description": "tqdm"
    },
	"enumerate_tqdm": {
		"prefix": "enumerate_tqdm",
		"body": [
			"for index, _ in enumerate(tqdm(_s)):",
            "    print(index, _)",
		],
		"description": "enumerate_tqdm"
    },
	"from tqdm import tqdm": {
		"prefix": "from tqdm import tqdm",
		"body": [
			"from tqdm import tqdm",
		],
		"description": "from tqdm import tqdm"
    },
	"enumerate": {
		"prefix": "enumerate",
		"body": [
            "for index, _ in enumerate(_s):",
            "    print(index, _)",
		],
		"description": "enumerate"
    },
    "enumerate_zip": {
		"prefix": "enumerate_zip",
		"body": [
            "for index, (_1, _2) in enumerate(zip(_1s, _2s)):",
            "    print(index, _1, _2)",
		],
		"description": "enumerate_zip"
    },
	"tqdm_zip": {
		"prefix": "tqdm_zip",
		"body": [
            "for (_1, _2) in tqdm(zip(_1s, _2s)):",
            "    print(_1, _2)"
		],
		"description": "tqdm_zip"
    },
	"enumerate_tqdm_zip": {
		"prefix": "enumerate_tqdm_zip",
		"body": [
            "for index, (_1, _2) in enumerate(tqdm(zip(_1s, _2s))):",
            "    print(index, _1, _2)",
		],
		"description": "enumerate_tqdm_zip"
    },
	"json load": {
		"prefix": "json load",
		"body": [
            "with open('json.json') as f:",
            "   _ = json.load(f)"
        ],
		"description": "json load"
    },
	"json dump": {
		"prefix": "json dump",
		"body": [
            "with open('_.json', 'w') as f:",
            "   json.dump(d, f, indent=4)"
        ],
		"description": "json dump"
	},
	"pathlib mkdir": {
		"prefix": "pathlib mkdir",
		"body": [
            "output_dir=Path('_')",
            "if output_dir.exists():",
            "    shutil.rmtree(output_dir.name)",
            "output_dir.mkdir(parents=True, exist_ok=True)"
        ],
		"description": "pathlib mkdir"
    },
	"pickle load": {
		"prefix": "pickle load",
		"body": [
        "with open('_.pkl', 'rb') as f:",
        "    _ = pickle.load(f)"
        ],
		"description": "pickle load"
    },
    "elappsed time": {
		"prefix": "elappsed time",
		"body": [
            "start_time = time()",
            "elapsed_time = time() - start_time",
            "print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")"       ],
		"description": "elappsed time"
    },
    "np where": {
		"prefix": "np where",
		"body": [
            "list(zip(*np.where(a < 4)))"
        ],
		"description": "np where 2dim"
    },
    "random sample": {
		"prefix": "random sample",
		"body": [
            "__ = random.sample(_, len(_))"
        ],
		"description": "random sample"
    },
    "pathlib mkdir 2dir": {
		"prefix": "pathlib mkdir 2dir",
		"body": [
            "output_root_dir=\"the_root_dir\"",
            "output__dir=Path(output_root_dir) / \"the_dir_1\"",
            "output___dir=Path(output_root_dir) / \"the_dir_2\"",
            "",
            "if output__dir.exists():",
            "    shutil.rmtree(str(output__dir))",
            "output__dir.mkdir(parents=True, exist_ok=True)",
            "",
            "if output___dir.exists():",
            "    shutil.rmtree(str(output___dir))",
            "output___dir.mkdir(parents=True, exist_ok=True)"
        ],
    "description": "pathlib mkdir 2dir"
    },
    "pathlib mkdir 2dir 2": {
		"prefix": "pathlib mkdir 2dir 2",
		"body": [
            "output_root_dir=Path(\"output_root_dir\")",
            "if output_root_dir.exists():",
            "    shutil.rmtree(output_root_dir)",
            "",
            "output_dir1=output_root_dir / \"_dir1\"",
            "output_dir2=output_root_dir / \"_dir2\"",
            "",
            "output_dir1.mkdir(parents=True, exist_ok=True)",   
            "output_dir2.mkdir(parents=True, exist_ok=True)"
        ],
        "description": "pathlib mkdir 2dir 2"
        },
}